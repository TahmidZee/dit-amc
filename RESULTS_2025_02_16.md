# Training Results & Analysis: February 16, 2025

## Executive Summary

Today we completed analysis of the A4/A5 ablation runs (`cldnn_a4_tuned_reg_mixup02` and `cldnn_a5_tuned_expert_snrbal_mixup02`), identified a critical issue with mixup augmentation interfering with denoiser training, and implemented a fix that isolates mixup to classification loss only. We are now running refined experiments to validate the improvement.

---

## 1. Results from A4/A5 Runs

### 1.1 Performance Summary

**A4 (`cldnn_a4_tuned_reg_mixup02`):**
- **Best validation accuracy:** `0.63930` at epoch 41
- **Current status (epoch 46):** `0.63933` — essentially flat since epoch 41
- **Low-SNR band (-14..-6 dB) mean:** `0.16485`
- **Mid-SNR band (-4..+6 dB) mean:** `0.6719`
- **High-SNR band (≥10 dB) mean:** `0.9692`

**A5 (`cldnn_a5_tuned_expert_snrbal_mixup02`):**
- **Best validation accuracy:** `0.63685` at epoch 41
- **Current status (epoch 43):** `0.63684` — flat
- **Low-SNR band (-14..-6 dB) mean:** `0.16303`
- **Mid-SNR band (-4..+6 dB) mean:** `0.6662`
- **High-SNR band (≥10 dB) mean:** `0.9677`

### 1.2 Comparison to Prior Ablations

| Run | Best Val Acc | Low-SNR Mean | Mid-SNR Mean | High-SNR Mean |
|-----|-------------|--------------|-------------|---------------|
| A3 (dual-path, no mixup) | 0.6397 | **0.1710** | 0.6604 | 0.9707 |
| A4 (dual-path + FiLM + mixup) | 0.6393 | 0.1648 | **0.6719** | 0.9692 |
| A5 (A4 + expert + snr-balanced) | 0.6368 | 0.1630 | 0.6662 | 0.9677 |

**Key observations:**
1. **A4/A5 did not improve overall** — best val_acc is essentially tied with A3
2. **Low-SNR regressed** — A3 had better low-SNR performance (`0.1710` vs `0.1648`/`0.1630`)
3. **Mid-SNR improved slightly** — A4 gained `+1.15pp` in mid-SNR vs A3
4. **A5 underperformed** — adding expert features + SNR-balanced sampling hurt overall performance

### 1.3 Why A4 Didn't Outperform A3 Despite Having Both Denoiser + FiLM

**The Puzzle:** A4 has both denoiser (from A3) and noise-fraction FiLM (from A1), yet it performs similarly to A3 overall and **worse** at low-SNR. Why?

**Configuration Differences:**

| Component | A3 | A4 |
|-----------|----|----|
| Denoiser | ✅ | ✅ |
| Dual-path | ✅ | ✅ |
| Noise-fraction FiLM | ❌ | ✅ |
| Mixup (alpha=0.2) | ❌ | ✅ |
| Dropout | 0.15 | 0.20 |
| Aug-shift | ❌ | ✅ |
| Lambda_id | 0.03 | 0.05 |

**Performance Breakdown:**

| Metric | A3 | A4 | Delta |
|--------|----|----|-------|
| Overall val_acc | 0.6397 | 0.6393 | **-0.0004** |
| Low-SNR (-14..-6) | 0.1710 | 0.1656 | **-0.0054 (-0.54pp)** |
| Mid-SNR (-4..+6) | 0.6604 | 0.6723 | **+0.0119 (+1.19pp)** |
| High-SNR (≥10) | 0.9707 | 0.9692 | -0.0015 |

**Root Causes:**

1. **Mixup corrupted denoiser training** (primary issue)
   - A4's denoiser saw mixed/non-physical targets → learned suboptimal noise removal
   - Low-SNR depends heavily on denoiser quality → explains `-0.54pp` regression
   - This **canceled out** the expected benefit from adding FiLM

2. **FiLM helped mid-SNR** (expected)
   - Noise-fraction conditioning is most useful in the transition zone (`-4..+6 dB`)
   - A4 gained `+1.19pp` in mid-SNR, confirming FiLM works when denoiser isn't corrupted

3. **Stronger regularization may have hurt low-SNR capacity**
   - A4: dropout `0.20` vs A3: `0.15` (+33% stronger)
   - A4: `aug_shift` enabled (additional augmentation)
   - Low-SNR needs more model capacity → stronger regularization can hurt

4. **Partial redundancy between denoiser and FiLM**
   - Both help with noise handling, but in different ways:
     - Denoiser: **explicit noise removal** (reconstruction)
     - FiLM: **adaptive feature scaling** (conditioning)
   - Adding both doesn't double the benefit; they partially overlap

5. **Interaction effects**
   - FiLM conditions on `eta_hat` predicted by the noise head
   - If denoiser is corrupted by mixup, noise head may also be affected
   - This creates a cascade: bad denoiser → bad noise estimates → bad FiLM conditioning

**Conclusion:** A4's mixup interference **masked** the FiLM benefit. The `+1.19pp` mid-SNR gain shows FiLM works, but the `-0.54pp` low-SNR loss (from corrupted denoiser) canceled it out overall. With classifier-only mixup, we expect A4-style runs to show **both** benefits: FiLM's mid-SNR gain **plus** clean denoiser's low-SNR recovery.

---

## 2. Root Cause Analysis: Mixup Interference

### 2.1 The Problem

**Original behavior (legacy):** Mixup was applied **before** all losses, including denoiser/noise auxiliary losses. This meant:
- Denoiser saw **mixed/non-physical IQ signals** as targets
- Noise-fraction head saw **mixed SNR labels** (no physical meaning)
- The denoiser was learning to denoise "fake" mixed samples rather than real noisy signals

**Why this hurts low-SNR performance:**
- The denoiser is critical for low-SNR recovery (`-14..-6 dB` band)
- If denoiser training is corrupted by non-physical mixup targets, it cannot learn proper noise removal
- This explains why A4/A5 (with mixup) had **worse low-SNR** than A3 (without mixup)

### 2.2 Evidence

1. **A3 (no mixup) had better low-SNR** despite missing FiLM conditioning
2. **A4/A5 plateaued early** — best epoch was 41, then flat (suggesting denoiser hit a ceiling)
3. **Mixup statistics:** With `alpha=0.2`, ~67% of batches are "near-pure" (lam ≤ 0.1 or ≥ 0.9), but the remaining 33% still corrupt denoiser supervision

---

## 3. Solution: Classifier-Only Mixup

### 3.1 Implementation

We modified `train.py` to support **classifier-only mixup**:

- **New CLI flags:**
  - `--mixup-cls-only` (default: ON) — apply mixup only to classification loss
  - `--mixup-all-losses` (legacy) — old behavior for comparison

- **Training loop changes:**
  - Classification loss uses **mixed inputs** (`x_cls`, `y_a`, `y_b`, `lam`)
  - Denoiser/noise/SNR auxiliary losses use **clean inputs** (`x_aux`, `snr_aux`)
  - This ensures denoiser always sees physically meaningful targets

### 3.2 Why This Should Help

1. **Denoiser gets clean supervision** — learns proper noise removal on real signals
2. **Classification still benefits from mixup** — regularization without corrupting auxiliary losses
3. **Low-SNR should improve** — denoiser quality directly impacts `-14..-6 dB` performance

---

## 4. Current Experiments

### 4.1 Run 1: Baseline Classifier-Only Mixup

**Screen 2:** `cldnn_a4_tuned_reg_mixup02_cls_only`
- Same recipe as A4, but with `--mixup-cls-only` enabled
- `mixup_alpha=0.2`, `mixup_prob=0.5` (unchanged)
- **Goal:** Validate that classifier-only mixup improves low-SNR without hurting overall

### 4.2 Run 2: Refined Classifier-Only Mixup (Planned)

**Screen 1:** (after stopping flat A4 run at epoch ~50)
- `cldnn_a4_tuned_reg_mixup03_snrmin6_cls_only`
- Same as Run 1, but with:
  - `mixup_prob=0.3` (reduced from 0.5)
  - `mixup_snr_min=6` (only mix cleaner samples)
- **Goal:** Further refine mixup to avoid corrupting even classification on very low-SNR samples

---

## 5. Expected Outcomes

### 5.1 Success Criteria

**Run 1 (cls-only baseline) should:**
- Match or exceed A4 overall val_acc (`≥0.6393`)
- **Improve low-SNR by ≥+1.0pp** vs A4 (`≥0.1748` mean in `-14..-6 dB`)
- Maintain high-SNR performance (`≥0.969`)

**Run 2 (refined cls-only) should:**
- Match or exceed Run 1
- Potentially improve low-SNR further if `snr_min=6` gate helps

### 5.2 If Successful

- **Promote classifier-only mixup as default** for all future denoiser runs
- **Re-evaluate A5** — expert features + SNR-balanced may work better with clean denoiser supervision
- **Proceed to `L_feat` implementation** (Section 15 of `ARCHITECTURE_PLAN_V3.md`)

### 5.3 If Unsuccessful

- Mixup may not be the primary bottleneck
- Consider:
  - Targeted degradation sampling (focus `Delta` on `-14..-6 dB` band)
  - Feature preservation loss (`L_feat`) to constrain denoiser
  - Alternative regularization (reduce mixup further or disable)

---

## 6. Technical Details

### 6.1 Code Changes

**File:** `train.py`

**Key modifications:**
1. Added `--mixup-cls-only` / `--mixup-all-losses` CLI arguments
2. Split input paths: `x_clean`/`snr_clean` for aux losses, `x_cls`/`snr_cls` for classification
3. Updated both AMP and non-AMP training paths
4. Fixed train accuracy logging to use clean logits when cls-only mixup is active

**Validation:**
- `python -m py_compile train.py` passes
- No linter errors
- CLI help confirms new flags are exposed

### 6.2 Mixup Statistics (alpha=0.2)

- **P[strongly blended 0.3..0.7]:** ~13.5%
- **P[near-pure ≤0.1 or ≥0.9]:** ~67.3%
- **Effective mixing strength:** E[min(lam, 1-lam)] ≈ 0.101

Even with moderate alpha, ~33% of batches have meaningful mixing that can corrupt denoiser targets.

---

## 7. Next Steps

1. **Monitor Run 1** (screen 2) — check low-SNR metrics at epoch ~20-25
2. **Stop screen 1 A4** at epoch 50 if still flat
3. **Launch Run 2** (screen 1) with refined mixup settings
4. **Compare results** — Run 1 vs Run 2 vs original A4
5. **Decide on `L_feat`** — proceed if mixup fix shows promise, or pivot if not

---

## 8. Lessons Learned

1. **Auxiliary losses need clean supervision** — mixing augmentation should not corrupt physically meaningful targets (denoiser, noise estimation, SNR consistency)
2. **Low-SNR is the hardest regime** — small improvements in denoiser quality can have outsized impact on `-14..-6 dB` performance
3. **Ablation order matters** — A3 (no mixup) gave a cleaner baseline; A4/A5 added complexity that masked the mixup issue
4. **Monitor low-SNR separately** — overall val_acc can hide low-SNR regressions if high-SNR dominates

---

## Appendix: Run Commands

### Run 1 (Baseline cls-only)
```bash
--mixup-alpha 0.2 --mixup-prob 0.5 --mixup-cls-only
--out-dir .../cldnn_a4_tuned_reg_mixup02_cls_only
```

### Run 2 (Refined cls-only)
```bash
--mixup-alpha 0.2 --mixup-prob 0.3 --mixup-snr-min 6 --mixup-cls-only
--out-dir .../cldnn_a4_tuned_reg_mixup03_snrmin6_cls_only
```

---

**Document created:** 2025-02-16  
**Status:** Active experiments in progress
